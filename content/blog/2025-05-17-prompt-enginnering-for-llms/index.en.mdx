---
title: Thoughts on "Prompt Engineering for LLMs"
date: 2025-05-17
category: Programming
tags: ["Programming", "LLM"]
draft: true
---

# Prompt Engineering for LLMs

## Chapter 1

### Paper references

- [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Language Models Are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Language Models Are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)

## Chapter 2

### References

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

## Chapter 3: Transition to chat format

### References

- [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/pdf/2203.02155)
- [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861)

## Chapter 4

### References

- [Language Models are Unsuper vised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629)
